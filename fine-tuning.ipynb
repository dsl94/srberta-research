{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BPE ensures that the most common words are represented in the vocabulary as a single token while the rare words are broken down into two or more subword tokens and this is in agreement with what a subword-based tokenization algorithm does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T13:59:18.122170Z",
     "start_time": "2023-07-01T13:59:18.111218500Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T13:59:20.964847200Z",
     "start_time": "2023-07-01T13:59:20.930003800Z"
    }
   },
   "outputs": [],
   "source": [
    "paths = [str(x) for x in Path('./train_data').glob('*.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T13:59:21.853794900Z",
     "start_time": "2023-07-01T13:59:21.784102700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['train_data\\\\legislativa0_train.txt',\n 'train_data\\\\legislativa1_test.txt',\n 'train_data\\\\legislativa1_train.txt',\n 'train_data\\\\legislativa2_train.txt',\n 'train_data\\\\legislativa3_train.txt',\n 'train_data\\\\legislativa4_train.txt',\n 'train_data\\\\legislativa5_train.txt',\n 'train_data\\\\legislativa6_train.txt']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Pipeline and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import os\n",
    "def divide_files(input_folder, output_folder, chunk_size):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Iterate over each file in the input folder\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            input_file = os.path.join(input_folder, filename)\n",
    "            output_file_base = os.path.splitext(filename)[0]\n",
    "\n",
    "            with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read().replace(\"\\n\", \"\")\n",
    "\n",
    "            # Divide content into chunks\n",
    "            chunks = [content[i:i + chunk_size] for i in range(0, len(content), chunk_size)]\n",
    "\n",
    "            # Save each chunk as a new file\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                output_file = os.path.join(output_folder, f\"legislativa_{i + 1}.txt\")\n",
    "                with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(chunk)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_folder = \"./raw_data\"\n",
    "output_folder = \"./train_data\"\n",
    "chunk_size = 1000000\n",
    "\n",
    "divide_files(input_folder, output_folder, chunk_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-26T07:40:46.577967500Z",
     "start_time": "2023-06-26T07:40:44.829104100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need 3 tensors:\n",
    "Labels tensor is our input ids tensor which represents the id in vocabulary for each token and they need to be passed throug mass language modelying system\n",
    "\n",
    "input_ids — our token_ids with ~15% of tokens masked using the mask token <mask>. We are not masking special tokens\n",
    "attention_mask — a tensor of 1s and 0s, marking the position of ‘real’ tokens/padding tokens — used in attention calculations.\n",
    "labels — our token_ids with no masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T13:59:32.822196200Z",
     "start_time": "2023-07-01T13:59:28.498910400Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def mlm(tensor):\n",
    "    \n",
    "    rand = torch.rand(tensor.shape) #[0,1]\n",
    "    mask_arr = (rand < 0.15)* (tensor!=0)* (tensor!=1)* (tensor!=2)\n",
    "    for i in range(tensor.shape[0]):\n",
    "        selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "        tensor[i, selection] = 4\n",
    "        \n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T13:59:34.639976500Z",
     "start_time": "2023-07-01T13:59:33.580569100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer_srberta = RobertaTokenizerFast.from_pretrained(\"../SRBerta-pretrain/srberta_tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T08:18:19.265720500Z",
     "start_time": "2023-06-26T08:15:22.490112600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/8 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "109bd01c12544a36b8a1c27e184b2fba"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "\n",
    "paths = [str(x) for x in Path('./train_data').glob('*.txt')]\n",
    "\n",
    "input_ids = []\n",
    "mask = [] \n",
    "labels = []\n",
    "\n",
    "for path in tqdm(paths):\n",
    "    with open(path, 'r', encoding='utf-8') as f:       \n",
    "        lines = f.read().split('\\n')\n",
    "        # print(lines[0])\n",
    "        \n",
    "    sample = tokenizer_srberta(lines, max_length=512, padding='max_length', truncation=True, return_tensors='pt')\n",
    "    labels.append(sample.input_ids)\n",
    "    mask.append(sample.attention_mask)\n",
    "    input_ids.append(mlm(sample.input_ids.detach().clone()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T08:18:22.266637400Z",
     "start_time": "2023-06-26T08:18:22.116369Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([7854, 512])"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T08:18:23.154165600Z",
     "start_time": "2023-06-26T08:18:23.104116700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "8"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that no matter how big one txt sample in dataset['train'][idx] is, it will get truncated to size 512!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load if enough RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-28T19:01:21.751352Z",
     "start_time": "2023-06-28T19:01:20.764234200Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m input_ids \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241m.\u001B[39mcat(input_ids)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "input_ids = torch.cat(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T08:18:27.223889400Z",
     "start_time": "2023-06-26T08:18:27.163853300Z"
    }
   },
   "outputs": [],
   "source": [
    "mask = torch.cat(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T08:18:27.801752200Z",
     "start_time": "2023-06-26T08:18:27.731735100Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = torch.cat(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T08:18:28.292040500Z",
     "start_time": "2023-06-26T08:18:28.262044800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "41833"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T08:18:29.098488900Z",
     "start_time": "2023-06-26T08:18:29.035570900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[    0,  1689,  9107,  ...,    16,   342,     2],\n        [    0,   456,  2296,  ...,   289,  9098,     2],\n        [    0,  3337,  1416,  ...,   955, 10053,     2],\n        ...,\n        [    0,   331, 11736,  ...,   551,  7076,     2],\n        [    0, 16932,   263,  ...,    18,  1289,     2],\n        [    0,   310,  1654,  ..., 16114,   510,     2]])"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save tensors to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T08:18:33.207793200Z",
     "start_time": "2023-06-26T08:18:33.177794400Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(input_ids, './train_data/input_ids.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T08:18:33.736574700Z",
     "start_time": "2023-06-26T08:18:33.435298600Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(mask, './train_data/mask.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T08:18:34.610660300Z",
     "start_time": "2023-06-26T08:18:34.359627900Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(labels, './train_data/labels.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tensor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T13:59:39.419919400Z",
     "start_time": "2023-07-01T13:59:39.161596800Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "input_ids = torch.load(\"./train_data/input_ids.pt\")\n",
    "mask = torch.load(\"./train_data/mask.pt\")\n",
    "labels = torch.load(\"./train_data/labels.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test masked language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids[0][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[0][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T13:59:42.819652700Z",
     "start_time": "2023-07-01T13:59:42.780820400Z"
    }
   },
   "outputs": [],
   "source": [
    "encodings = {\n",
    "    'input_ids': input_ids,\n",
    "    'attention_mask': mask,\n",
    "    'labels': labels\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T13:59:43.646005100Z",
     "start_time": "2023-07-01T13:59:43.606175300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create Dataset object\n",
    "import torch\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.encodings['input_ids'].shape[0]\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return {key: tensor[i] for key, tensor in self.encodings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T13:59:45.202537300Z",
     "start_time": "2023-07-01T13:59:45.167695Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = Dataset(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T13:59:45.600506200Z",
     "start_time": "2023-07-01T13:59:45.578558300Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 12\n",
    "DO_SHUFFLE = True\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=DO_SHUFFLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T13:59:46.733450400Z",
     "start_time": "2023-07-01T13:59:46.696613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41833\n"
     ]
    }
   ],
   "source": [
    "print(len(dataloader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T13:59:52.025099800Z",
     "start_time": "2023-07-01T13:59:51.521039Z"
    }
   },
   "outputs": [],
   "source": [
    "#tensorboard --logdir=runs\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(\"./runs_fine_tunning_6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load trained model and optimizer checkpoint if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T05:57:36.808610600Z",
     "start_time": "2023-06-30T05:57:35.579123400Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device.cpu()\n",
    "print(str(device))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-30T15:34:55.752803400Z",
     "start_time": "2023-06-30T15:34:55.730899800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-30T05:57:41.632204400Z",
     "start_time": "2023-06-30T05:57:39.624706700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "RobertaForMaskedLM(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (lm_head): RobertaLMHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n  )\n)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaForMaskedLM.from_pretrained(\"../SRBerta-pretrain/srberta_model_24\")\n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "checkpoint = torch.load('../SRBerta-pretrain/24_optimizer.pt',map_location=device)\n",
    "optim = AdamW(params=model.parameters())\n",
    "print(optim)\n",
    "optim.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "print(optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "models = [5,10,14,18,24]\n",
    "for m in range(1):\n",
    "\n",
    "        \n",
    "    from transformers import RobertaForMaskedLM\n",
    "    model = RobertaForMaskedLM.from_pretrained(f\"../SRBerta-pretrain/srberta_model_24\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device.cpu()\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    from transformers import AdamW\n",
    "    optim = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    \n",
    "    step=0\n",
    "    num_epochs=5\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        loop = tqdm(dataloader, leave=True)\n",
    "\n",
    "        for batch in loop:\n",
    "\n",
    "            optim.zero_grad()\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask=mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            loop.set_description(f'Epoch: {epoch}')\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "            writer.add_scalar(f\"model24_test/step\", loss, step)\n",
    "            writer.flush()\n",
    "\n",
    "            step+=1\n",
    "\n",
    "        # Save after each epoch\n",
    "        model.save_pretrained(f\"./fine_tuned/model_24_epoch_\"+ str(epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
